{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "- https://aws.amazon.com/blogs/aws/announcing-aws-glue-databrew-a-visual-data-preparation-tool-that-helps-you-clean-and-normalize-data-faster/\n",
    "- https://github.com/aws-samples/aws-glue-samples\n",
    "- https://github.com/awslabs/aws-glue-libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is AWS Glue?\n",
    "https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html\n",
    "\n",
    "AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so thereâ€™s no infrastructure to set up or manage.\n",
    "\n",
    "AWS Glue is designed to work with semi-structured data. It introduces a component called a dynamic frame, which you can use in your ETL scripts. A dynamic frame is similar to an Apache Spark dataframe, which is a data abstraction used to organize data into rows and columns, except that each record is self-describing so no schema is required initially. With dynamic frames, you get schema flexibility and a set of advanced transformations specifically designed for dynamic frames. You can convert between dynamic frames and Spark dataframes, so that you can take advantage of both AWS Glue and Spark transformations to do the kinds of analysis that you want.\n",
    "\n",
    "You can use the AWS Glue console to discover data, transform it, and make it available for search and querying. The console calls the underlying services to orchestrate the work required to transform your data. You can also use the AWS Glue API operations to interface with AWS Glue services. Edit, debug, and test your Python or Scala Apache Spark ETL code using a familiar development environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Should I Use AWS Glue?\n",
    "__You can use AWS Glue to organize, cleanse, validate, and format data for storage in a data warehouse or data lake__. You can transform and move AWS Cloud data into your data store. You can also load data from disparate static or streaming data sources into your data warehouse or data lake for regular reporting and analysis. By storing data in a data warehouse or data lake, you integrate information from different parts of your business and provide a common source of data for decision making.\n",
    "\n",
    "- AWS Glue simplifies many tasks when you are building a data warehouse or data lake:\n",
    "\n",
    "- Discovers and catalogs metadata about your data stores into a central catalog. You can process semi-structured data, such as clickstream or process logs.\n",
    "\n",
    "- Populates the AWS Glue Data Catalog with table definitions from scheduled crawler programs. Crawlers call classifier logic to infer the schema, format, and data types of your data. This metadata is stored as tables in the AWS Glue Data Catalog and used in the authoring process of your ETL jobs.\n",
    "\n",
    "- Generates ETL scripts to transform, flatten, and enrich your data from source to target.\n",
    "\n",
    "- Detects schema changes and adapts based on your preferences.\n",
    "\n",
    "- Triggers your ETL jobs based on a schedule or event. You can initiate jobs automatically to move your data into your data warehouse or data lake. Triggers can be used to create a dependency flow between jobs.\n",
    "\n",
    "- Gathers runtime metrics to monitor the activities of your data warehouse or data lake.\n",
    "\n",
    "- Handles errors and retries automatically.\n",
    "\n",
    "- Scales resources, as needed, to run your jobs.\n",
    "\n",
    "__You can use AWS Glue when you run serverless queries against your Amazon S3 data lake__. AWS Glue can catalog your Amazon Simple Storage Service (Amazon S3) data, making it available for querying with Amazon Athena and Amazon Redshift Spectrum. With crawlers, your metadata stays in sync with the underlying data. Athena and Redshift Spectrum can directly query your Amazon S3 data lake using the AWS Glue Data Catalog. With AWS Glue, you access and analyze data through one unified interface without loading it into multiple data silos.\n",
    "\n",
    "__You can create event-driven ETL pipelines with AWS Glue__. You can run your ETL jobs as soon as new data becomes available in Amazon S3 by invoking your AWS Glue ETL jobs from an AWS Lambda function. You can also register this new dataset in the AWS Glue Data Catalog as part of your ETL jobs.\n",
    "\n",
    "__You can use AWS Glue to understand your data assets__. You can store your data using various AWS services and still maintain a unified view of your data using the AWS Glue Data Catalog. View the Data Catalog to quickly search and discover the datasets that you own, and maintain the relevant metadata in one central repository. The Data Catalog also serves as a drop-in replacement for your external Apache Hive Metastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue: How It Works\n",
    "https://docs.aws.amazon.com/glue/latest/dg/how-it-works.html\n",
    "\n",
    "AWS Glue uses other AWS services to orchestrate your ETL (extract, transform, and load) jobs to build data warehouses and data lakes and generate output streams. AWS Glue calls API operations to transform your data, create runtime logs, store your job logic, and create notifications to help you monitor your job runs. The AWS Glue console connects these services into a managed application, so you can focus on creating and monitoring your ETL work. The console performs administrative and job development operations on your behalf. You supply credentials and other properties to AWS Glue to access your data sources and write to your data targets.\n",
    "\n",
    "AWS Glue takes care of provisioning and managing the resources that are required to run your workload. You don't need to create the infrastructure for an ETL tool because AWS Glue does it for you. When resources are required, to reduce startup time, AWS Glue uses an instance from its warm pool of instances to run your workload.\n",
    "\n",
    "With AWS Glue, you create jobs using table definitions in your Data Catalog. Jobs consist of scripts that contain the programming logic that performs the transformation. You use triggers to initiate jobs either on a schedule or as a result of a specified event. You determine where your target data resides and which source data populates your target. With your input, AWS Glue generates the code that's required to transform your data from source to target. You can also provide scripts in the AWS Glue console or API to process your data.\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "AWS Glue supports the following data sources:\n",
    "\n",
    "Data stores\n",
    "- Amazon S3\n",
    "- Amazon Relational Database Service (Amazon RDS)\n",
    "- Third-party JDBC-accessible databases\n",
    "- Amazon DynamoDB\n",
    "\n",
    "Data streams\n",
    "- Amazon Kinesis Data Streams\n",
    "- Apache Kafka\n",
    "\n",
    "### Data Targets\n",
    "\n",
    "AWS Glue supports the following data targets:\n",
    "\n",
    "Data stores\n",
    "- Amazon S3\n",
    "- Amazon Relational Database Service (Amazon RDS)\n",
    "- Third-party JDBC-accessible databases\n",
    "\n",
    "AWS Glue is available in several AWS Regions. For more information, see [AWS Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html) in the Amazon Web Services General Reference.\n",
    "\n",
    "### Topics\n",
    "\n",
    "- [Serverless ETL Jobs Run in Isolation](https://docs.aws.amazon.com/glue/latest/dg/how-it-works.html#how-it-works-isolation)\n",
    "- [AWS Glue Concepts](https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html)\n",
    "- [AWS Glue Components](https://docs.aws.amazon.com/glue/latest/dg/components-overview.html)\n",
    "- [Converting Semi-Structured Schemas to Relational Schemas](https://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html)\n",
    "\n",
    "## Serverless ETL Jobs Run in Isolation\n",
    "AWS Glue runs your ETL jobs in an Apache Spark serverless environment. AWS Glue runs these jobs on virtual resources that it provisions and manages in its own service account.\n",
    "\n",
    "AWS Glue is designed to do the following:\n",
    "\n",
    "- Segregate customer data.\n",
    "\n",
    "- Protect customer data in transit and at rest.\n",
    "\n",
    "- Access customer data only as needed in response to customer requests, using temporary, scoped-down credentials, or with a customer's consent to IAM roles in their account.\n",
    "\n",
    "During provisioning of an ETL job, you provide input data sources and output data targets in your virtual private cloud (VPC). In addition, you provide the IAM role, VPC ID, subnet ID, and security group that are needed to access data sources and targets. For each tuple (customer account ID, IAM role, subnet ID, and security group), AWS Glue creates a new Spark environment that is isolated at the network and management level from all other Spark environments inside the AWS Glue service account.\n",
    "\n",
    "AWS Glue creates elastic network interfaces in your subnet using private IP addresses. Spark jobs use these elastic network interfaces to access your data sources and data targets. Traffic in, out, and within the Spark environment is governed by your VPC and networking policies with one exception: Calls made to AWS Glue libraries can proxy traffic to AWS Glue API operations through the AWS Glue VPC. All AWS Glue API calls are logged; thus, data owners can audit API access by enabling [AWS CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/), which delivers audit logs to your account.\n",
    "\n",
    "AWS Glue managed Spark environments that run your ETL jobs are protected with the same security practices followed by other AWS services. Those practices are listed in the __AWS Access__ section of the [Introduction to AWS Security Processes whitepaper](https://d1.awsstatic.com/whitepapers/Security/Intro_Security_Practices.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue Concepts\n",
    "https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\n",
    "\n",
    "The following diagram shows the architecture of an AWS Glue environment.\n",
    "<img src=\"https://docs.aws.amazon.com/glue/latest/dg/images/HowItWorks-overview.png\" align=\"left\" alt=\"Glue Concept image\" width = \"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You define jobs in AWS Glue to accomplish the work that's required to extract, transform, and load (ETL) data from a data source to a data target. You typically perform the following actions:\n",
    "\n",
    "- For data store sources, you define a crawler to populate your AWS Glue Data Catalog with metadata table definitions. You point your crawler at a data store, and the crawler creates table definitions in the Data Catalog. For streaming sources, you manually define Data Catalog tables and specify data stream properties.\n",
    "\n",
    "In addition to table definitions, the AWS Glue Data Catalog contains other metadata that is required to define ETL jobs. You use this metadata when you define a job to transform your data.\n",
    "\n",
    "- AWS Glue can generate a script to transform your data. Or, you can provide the script in the AWS Glue console or API.\n",
    "\n",
    "- You can run your job on demand, or you can set it up to start when a specified trigger occurs. The trigger can be a time-based schedule or an event.\n",
    "\n",
    "When your job runs, a script extracts data from your data source, transforms the data, and loads it to your data target. The script runs in an Apache Spark environment in AWS Glue.\n",
    "\n",
    "__!Important__\n",
    "Tables and databases in AWS Glue are objects in the AWS Glue Data Catalog. They contain metadata; they don't contain data from a data store.\n",
    "Text-based data, such as CSVs, must be encoded in UTF-8 for AWS Glue to process it successfully. For more information, see [UTF-8](https://en.wikipedia.org/wiki/UTF-8) in Wikipedia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue Terminology\n",
    "AWS Glue relies on the interaction of several components to create and manage your extract, transfer, and load (ETL) workflow.\n",
    "\n",
    "### AWS Glue Data Catalog\n",
    "The persistent metadata store in AWS Glue. It contains table definitions, job definitions, and other control information to manage your AWS Glue environment. Each AWS account has one AWS Glue Data Catalog per region.\n",
    "\n",
    "### Classifier\n",
    "Determines the schema of your data. AWS Glue provides classifiers for common file types, such as CSV, JSON, AVRO, XML, and others. It also provides classifiers for common relational database management systems using a JDBC connection. You can write your own classifier by using a grok pattern or by specifying a row tag in an XML document.\n",
    "\n",
    "### Connection\n",
    "A Data Catalog object that contains the properties that are required to connect to a particular data store.\n",
    "\n",
    "### Crawler\n",
    "A program that connects to a data store (source or target), progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog.\n",
    "\n",
    "### Database\n",
    "A set of associated Data Catalog table definitions organized into a logical group.\n",
    "\n",
    "### Data store, data source, data target\n",
    "A data store is a repository for persistently storing your data. Examples include Amazon S3 buckets and relational databases. A data source is a data store that is used as input to a process or transform. A data target is a data store that a process or transform writes to.\n",
    "\n",
    "### Development endpoint\n",
    "An environment that you can use to develop and test your AWS Glue ETL scripts.\n",
    "\n",
    "### Dynamic Frame\n",
    "A distributed table that supports nested data such as structures and arrays. Each record is self-describing, designed for schema flexibility with semi-structured data. Each record contains both data and the schema that describes that data. You can use both dynamic frames and Apache Spark dataframes in your ETL scripts, and convert between them. Dynamic frames provide a set of advanced transformations for data cleaning and ETL.\n",
    "\n",
    "### Job\n",
    "The business logic that is required to perform ETL work. It is composed of a transformation script, data sources, and data targets. Job runs are initiated by triggers that can be scheduled or triggered by events.\n",
    "\n",
    "### Notebook server\n",
    "A web-based environment that you can use to run your PySpark statements. PySpark is a Python dialect for ETL programming. For more information, see Apache Zeppelin. You can set up a notebook server on a development endpoint to run PySpark statements with AWS Glue extensions.\n",
    "\n",
    "### Script\n",
    "Code that extracts data from sources, transforms it, and loads it into targets. AWS Glue generates PySpark or Scala scripts.\n",
    "\n",
    "### Table\n",
    "The metadata definition that represents your data. Whether your data is in an Amazon Simple Storage Service (Amazon S3) file, an Amazon Relational Database Service (Amazon RDS) table, or another set of data, a table defines the schema of your data. A table in the AWS Glue Data Catalog consists of the names of columns, data type definitions, partition information, and other metadata about a base dataset. The schema of your data is represented in your AWS Glue table definition. The actual data remains in its original data store, whether it be in a file or a relational database table. AWS Glue catalogs your files and relational database tables in the AWS Glue Data Catalog. They are used as sources and targets when you create an ETL job.\n",
    "\n",
    "### Transform\n",
    "The code logic that is used to manipulate your data into a different format.\n",
    "\n",
    "### Trigger\n",
    "Initiates an ETL job. Triggers can be defined based on a scheduled time or an event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue Components\n",
    "https://docs.aws.amazon.com/glue/latest/dg/components-overview.html\n",
    "\n",
    "AWS Glue provides a console and API operations to set up and manage your extract, transform, and load (ETL) workload. You can use API operations through several language-specific SDKs and the AWS Command Line Interface (AWS CLI). For information about using the AWS CLI, see [AWS CLI Command Reference](https://docs.aws.amazon.com/cli/latest/reference/).\n",
    "\n",
    "AWS Glue uses the AWS Glue Data Catalog to store metadata about data sources, transforms, and targets. The Data Catalog is a drop-in replacement for the Apache Hive Metastore. The AWS Glue Jobs system provides a managed infrastructure for defining, scheduling, and running ETL operations on your data. For more information about the AWS Glue API, see [AWS Glue API](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api.html).\n",
    "\n",
    "### AWS Glue Console\n",
    "You use the AWS Glue console to define and orchestrate your ETL workflow. The console calls several API operations in the AWS Glue Data Catalog and AWS Glue Jobs system to perform the following tasks:\n",
    "\n",
    "- Define AWS Glue objects such as jobs, tables, crawlers, and connections.\n",
    "- Schedule when crawlers run.\n",
    "- Define events or schedules for job triggers.\n",
    "- Search and filter lists of AWS Glue objects.\n",
    "- Edit transformation scripts.\n",
    "\n",
    "### AWS Glue Data Catalog\n",
    "The AWS Glue Data Catalog is your persistent metadata store. It is a managed service that lets you store, annotate, and share metadata in the AWS Cloud in the same way you would in an Apache Hive metastore.\n",
    "\n",
    "Each AWS account has one AWS Glue Data Catalog per AWS region. It provides a uniform repository where disparate systems can store and find metadata to keep track of data in data silos, and use that metadata to query and transform the data.\n",
    "\n",
    "You can use AWS Identity and Access Management (IAM) policies to control access to the data sources managed by the AWS Glue Data Catalog. These policies allow different groups in your enterprise to safely publish data to the wider organization while protecting sensitive information. IAM policies let you clearly and consistently define which users have access to which data, regardless of its location.\n",
    "\n",
    "The Data Catalog also provides comprehensive audit and governance capabilities, with schema change tracking and data access controls. You can audit changes to data schemas. This helps ensure that data is not inappropriately modified or inadvertently shared.\n",
    "\n",
    "For information about how to use the AWS Glue Data Catalog, see Populating the [AWS Glue Data Catalog](https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html). For information about how to program using the Data Catalog API, see Catalog API.\n",
    "\n",
    "The following are other AWS services and open source projects that use the AWS Glue Data Catalog:\n",
    "\n",
    "- AWS Lake Formation â€“ for more information, see What Is [AWS Lake Formation](https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html)? in the AWS Lake Formation Developer Guide.\n",
    "- Amazon Athena â€“ for more information, see [Understanding Tables, Databases, and the Data Catalog](https://docs.aws.amazon.com/athena/latest/ug/understanding-tables-databases-and-the-data-catalog.html) in the Amazon Athena User Guide.\n",
    "- Amazon Redshift Spectrum â€“ for more information, see [Using Amazon Redshift Spectrum to Query External Data](https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html) in the Amazon Redshift Database Developer Guide.\n",
    "- Amazon EMR â€“ for more information, see [Use Resource-Based Policies for Amazon EMR Access to AWS Glue Data Catalog](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-iam-roles-glue.html) in the Amazon EMR Management Guide.\n",
    "- AWS Glue Data Catalog Client for Apache Hive Metastore â€“ for more information about this GitHub project, see [AWS Glue Data Catalog Client for Apache Hive Metastore](https://github.com/awslabs/aws-glue-data-catalog-client-for-apache-hive-metastore).\n",
    "\n",
    "### AWS Glue Crawlers and Classifiers\n",
    "AWS Glue also lets you set up crawlers that can scan data in all kinds of repositories, classify it, extract schema information from it, and store the metadata automatically in the AWS Glue Data Catalog. The AWS Glue Data Catalog can then be used to guide ETL operations.\n",
    "\n",
    "For information about how to set up crawlers and classifiers, see [Defining Crawlers](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html). For information about how to program crawlers and classifiers using the AWS Glue API, see [Crawlers and Classifiers API](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler.html).\n",
    "\n",
    "### AWS Glue ETL Operations\n",
    "Using the metadata in the Data Catalog, AWS Glue can autogenerate Scala or PySpark (the Python API for Apache Spark) scripts with AWS Glue extensions that you can use and modify to perform various ETL operations. For example, you can extract, clean, and transform raw data, and then store the result in a different repository, where it can be queried and analyzed. Such a script might convert a CSV file into a relational form and save it in Amazon Redshift.\n",
    "\n",
    "For more information about how to use AWS Glue ETL capabilities, see Programming [ETL Scripts](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming.html).\n",
    "\n",
    "### Streaming ETL in AWS Glue\n",
    "AWS Glue enables you to perform ETL operations on streaming data using continuously-running jobs. AWS Glue streaming ETL is built on the Apache Spark Structured Streaming engine, and can ingest streams from Amazon Kinesis Data Streams, Apache Kafka, and Amazon Managed Streaming for Apache Kafka (Amazon MSK). Streaming ETL can clean and transform streaming data and load it into Amazon S3 or JDBC data stores. Use Streaming ETL in AWS Glue to process event data like IoT streams, clickstreams, and network logs.\n",
    "\n",
    "If you know the schema of the streaming data source, you can specify it in a Data Catalog table. If not, you can enable schema detection in the streaming ETL job. The job then automatically determines the schema from the incoming data.\n",
    "\n",
    "The streaming ETL job can use both AWS Glue built-in transforms and transforms that are native to Apache Spark Structured Streaming. For more information, see [Operations on streaming DataFrames/Datasets on the Apache Spark website](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#operations-on-streaming-dataframesdatasets).\n",
    "\n",
    "For more information, see [Adding Streaming ETL Jobs in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/add-job-streaming.html).\n",
    "\n",
    "### The AWS Glue Jobs System\n",
    "The AWS Glue Jobs system provides managed infrastructure to orchestrate your ETL workflow. You can create jobs in AWS Glue that automate the scripts you use to extract, transform, and transfer data to different locations. Jobs can be scheduled and chained, or they can be triggered by events such as the arrival of new data.\n",
    "\n",
    "For more information about using the AWS Glue Jobs system, see [Running and Monitoring AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/monitor-glue.html). For information about programming using the AWS Glue Jobs system API, see [Jobs API](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Semi-Structured Schemas to Relational Schemas\n",
    "https://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html\n",
    "\n",
    "It's common to want to convert semi-structured data into relational tables. Conceptually, you are flattening a hierarchical schema to a relational schema. AWS Glue can perform this conversion for you on-the-fly.\n",
    "\n",
    "Semi-structured data typically contains mark-up to identify entities within the data. It can have nested data structures with no fixed schema. For more information about semi-structured data, see Semi-structured data in Wikipedia.\n",
    "\n",
    "Relational data is represented by tables that consist of rows and columns. Relationships between tables can be represented by a primary key (PK) to foreign key (FK) relationship. For more information, see Relational database in Wikipedia.\n",
    "\n",
    "AWS Glue uses crawlers to infer schemas for semi-structured data. It then transforms the data to a relational schema using an ETL (extract, transform, and load) job. For example, you might want to parse JSON data from Amazon Simple Storage Service (Amazon S3) source files to Amazon Relational Database Service (Amazon RDS) tables. Understanding how AWS Glue handles the differences between schemas can help you understand the transformation process.\n",
    "\n",
    "This diagram shows how AWS Glue transforms a semi-structured schema to a relational schema.\n",
    "\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/glue/latest/dg/images/HowItWorks-schemaconversion.png\" align=\"left\" alt=\"schema conversion image\" width = \"600\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Flow showing conversion from semi-structured to relational schema.\n",
    "    \n",
    "The diagram illustrates the following:\n",
    "\n",
    "- Single value A converts directly to a relational column.\n",
    "\n",
    "- The pair of values, B1 and B2, convert to two relational columns.\n",
    "\n",
    "- Structure C, with children X and Y, converts to two relational columns.\n",
    "\n",
    "- Array D[] converts to a relational column with a foreign key (FK) that points to another relational table. Along with a primary key (PK), the second relational table has columns that contain the offset and value of the items in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up IAM Permissions for AWS Glue\n",
    "https://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html\n",
    "\n",
    "You use AWS Identity and Access Management (IAM) to define policies and roles that are needed to access resources used by AWS Glue. The following steps lead you through the basic permissions that you need to set up your environment. Depending on your business needs, you might have to add or reduce access to your resources.\n",
    "\n",
    "1. [Create an IAM Policy for the AWS Glue Service:](https://docs.aws.amazon.com/glue/latest/dg/create-service-policy.html) Create a service policy that allows access to AWS Glue resources.\n",
    "\n",
    "2. [Create an IAM Role for AWS Glue:](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html) Create an IAM role, and attach the AWS Glue service policy and a policy for your Amazon Simple Storage Service (Amazon S3) resources that are used by AWS Glue.\n",
    "\n",
    "3. [Attach a Policy to IAM Users That Access AWS Glue:](https://docs.aws.amazon.com/glue/latest/dg/attach-policy-iam-user.html) Attach policies to any IAM user that signs in to the AWS Glue console.\n",
    "\n",
    "4. [Create an IAM Policy for Notebooks:](https://docs.aws.amazon.com/glue/latest/dg/create-notebook-policy.html) Create a notebook server policy to use in the creation of notebook servers on development endpoints.\n",
    "\n",
    "5. [Create an IAM Role for Notebooks:](https://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html) Create an IAM role and attach the notebook server policy.\n",
    "\n",
    "6. [Create an IAM Policy for Amazon SageMaker Notebooks:](https://docs.aws.amazon.com/glue/latest/dg/create-sagemaker-notebook-policy.html) Create an IAM policy to use when creating Amazon SageMaker notebooks on development endpoints.\n",
    "\n",
    "7. [Create an IAM Role for Amazon SageMaker Notebooks:](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role-sagemaker-notebook.html) Create an IAM role and attach the policy to grant permissions when creating Amazon SageMaker notebooks on development endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
